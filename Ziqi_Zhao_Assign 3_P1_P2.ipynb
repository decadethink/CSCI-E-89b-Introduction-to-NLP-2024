{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e1ade7-faa1-40fb-9475-e7388f3baf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec5d9fb-33df-449d-be5e-233750b5cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text from the file\n",
    "with open('movie_review.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# NLTK processing\n",
    "# Tokenization\n",
    "tokens_nltk = word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems_nltk = [stemmer.stem(token) for token in tokens_nltk]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas_nltk = [lemmatizer.lemmatize(token) for token in tokens_nltk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bda85db-6090-4358-8c3a-b33f3cd05813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8472906c-dc8a-44e1-a416-6d5032754f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy processing\n",
    "import spacy\n",
    "\n",
    "# Load English model for SpaCy and processing text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# Stemming is not directly supported in SpaCy\n",
    "\n",
    "# Lemmatization\n",
    "lemmas_spacy = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6624e43e-c57a-4ef9-aa46-58744b384be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens (NLTK): ['Review', ':', '``', 'The', 'Dark', 'Knight', \"''\", ';', 'OR', 'Can', 'Superhero', 'Movie', 'Fatigue', 'Be', 'Defeated', '?', 'Glenn', 'Kenny', 'This', 'may', 'seem', 'like', 'faint', 'praise', ',', 'but', 'about', 'the', 'highest', 'compliment', 'I', 'can', 'give', 'Christopher', 'Nolan', '’', 's', 'The', 'Dark', 'Knight', 'right', 'now', 'is', 'to', 'say', 'that', 'there', 'were', 'many', 'long']\n",
      "\n",
      "\n",
      "Sample tokens (SpaCy): ['Review', ':', '\"', 'The', 'Dark', 'Knight', '\"', ';', 'OR', 'Can', 'Superhero', 'Movie', 'Fatigue', 'Be', 'Defeated', '?', '\\n\\n', 'Glenn', 'Kenny', '\\n\\n', 'This', 'may', 'seem', 'like', 'faint', 'praise', ',', 'but', 'about', 'the', 'highest', 'compliment', 'I', 'can', 'give', 'Christopher', 'Nolan', '’s', 'The', 'Dark', 'Knight', 'right', 'now', 'is', 'to', 'say', 'that', 'there', 'were', 'many']\n"
     ]
    }
   ],
   "source": [
    "# Printing first 50 of results index for comparison\n",
    "# Tokenization results\n",
    "print(\"Sample tokens (NLTK):\", tokens_nltk[:50])\n",
    "print(\"\\n\")\n",
    "print(\"Sample tokens (SpaCy):\", tokens_spacy[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde82c88-5a1f-4fc7-bc02-696827eba0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample stems (NLTK): ['review', ':', '``', 'the', 'dark', 'knight', \"''\", ';', 'or', 'can', 'superhero', 'movi', 'fatigu', 'be', 'defeat', '?', 'glenn', 'kenni', 'thi', 'may', 'seem', 'like', 'faint', 'prais', ',', 'but', 'about', 'the', 'highest', 'compliment', 'i', 'can', 'give', 'christoph', 'nolan', '’', 's', 'the', 'dark', 'knight', 'right', 'now', 'is', 'to', 'say', 'that', 'there', 'were', 'mani', 'long']\n"
     ]
    }
   ],
   "source": [
    "# Stemming results (not supported in SpaCy)\n",
    "print(\"Sample stems (NLTK):\", stems_nltk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b27a7aa-a7fc-448e-bbde-d3bb5633eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lemmas (NLTK): ['Review', ':', '``', 'The', 'Dark', 'Knight', \"''\", ';', 'OR', 'Can', 'Superhero', 'Movie', 'Fatigue', 'Be', 'Defeated', '?', 'Glenn', 'Kenny', 'This', 'may', 'seem', 'like', 'faint', 'praise', ',', 'but', 'about', 'the', 'highest', 'compliment', 'I', 'can', 'give', 'Christopher', 'Nolan', '’', 's', 'The', 'Dark', 'Knight', 'right', 'now', 'is', 'to', 'say', 'that', 'there', 'were', 'many', 'long']\n",
      "\n",
      "\n",
      "Sample lemmas (SpaCy): ['review', ':', '\"', 'the', 'Dark', 'Knight', '\"', ';', 'or', 'can', 'Superhero', 'Movie', 'Fatigue', 'be', 'defeat', '?', '\\n\\n', 'Glenn', 'Kenny', '\\n\\n', 'this', 'may', 'seem', 'like', 'faint', 'praise', ',', 'but', 'about', 'the', 'high', 'compliment', 'I', 'can', 'give', 'Christopher', 'Nolan', '’s', 'the', 'Dark', 'Knight', 'right', 'now', 'be', 'to', 'say', 'that', 'there', 'be', 'many']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization results\n",
    "print(\"Sample lemmas (NLTK):\", lemmas_nltk[:50])\n",
    "print(\"\\n\")\n",
    "print(\"Sample lemmas (SpaCy):\", lemmas_spacy[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29535e-1c8a-4cb6-9ba6-d5db77c11b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33324845-8a31-48d0-a05d-9c5a492d669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67935358-c470-4948-b3ab-0a7ec875efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample lemmas large text: ['[', 'pp', '38-40', ':', 'article', 'from', 'die', 'zeit', ',', '30', 'november', '1984', ',', 'by', 'thomas', 'von', 'randow', ']', 'bildschirmtext', ':']\n",
      "\n",
      "Sample lemmas small text: ['the', 'superhero', 'genre', 'ha', 'evolved', 'tremendously', ',', 'and', 'many', 'new', 'director', 'are', 'attempting', 'fresh', 'take', '.', 'despite', 'superhero', 'fatigue', ',']\n",
      "\n",
      "BoW model for smaller text: Counter({'the': 2, 'superhero': 2, ',': 2, '.': 2, 'genre': 1, 'ha': 1, 'evolved': 1, 'tremendously': 1, 'and': 1, 'many': 1, 'new': 1, 'director': 1, 'are': 1, 'attempting': 1, 'fresh': 1, 'take': 1, 'despite': 1, 'fatigue': 1, 'audience': 1, \"'s\": 1, 'love': 1, 'for': 1, 'action-packed': 1, 'movie': 1, 'seems': 1, 'unrelenting': 1})\n",
      "\n",
      "New words in the smaller text: {'superhero': 2, 'genre': 1, 'evolved': 1, 'tremendously': 1, 'director': 1, 'attempting': 1, 'fresh': 1, 'fatigue': 1, 'action-packed': 1, 'movie': 1, 'seems': 1, 'unrelenting': 1}\n",
      "\n",
      "Number of new words in the smaller text: 12\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Load the large text dataset, a very old txt I found online\n",
    "with open('long_article.txt', 'r', encoding='latin1') as file:\n",
    "    large_text = file.read()\n",
    "\n",
    "# Define a smaller text sample with new words, I made one from the movie review article\n",
    "small_text = \"\"\"\n",
    "The superhero genre has evolved tremendously, and many new directors are attempting fresh takes.\n",
    "Despite superhero fatigue, the audience's love for action-packed movies seems unrelenting.\n",
    "\"\"\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize and lemmatize  \n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "large_text_lemmas = tokenize_and_lemmatize(large_text)\n",
    "small_text_lemmas = tokenize_and_lemmatize(small_text)\n",
    "\n",
    "print(\"\\nSample lemmas large text:\", large_text_lemmas[:20])\n",
    "print(\"\\nSample lemmas small text:\", small_text_lemmas[:20])\n",
    "\n",
    "# Create vocabularies\n",
    "vocab_large_text = Counter(large_text_lemmas)\n",
    "bow_small_text = Counter(small_text_lemmas)\n",
    "\n",
    "# Display 1-grams BoW model for small text\n",
    "print(\"\\nBoW model for smaller text:\", bow_small_text)\n",
    "\n",
    "# Identify new words by comparing the smaller text to the large text's vocabulary\n",
    "new_words = [word for word in bow_small_text if word not in vocab_large_text]\n",
    "new_words_count = {word: bow_small_text[word] for word in new_words}\n",
    "\n",
    "# Display the new words and their counts\n",
    "print(\"\\nNew words in the smaller text:\", new_words_count)\n",
    "print(\"\\nNumber of new words in the smaller text:\", len(new_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb39fd-8b16-4d9f-ae9f-8528c39d4e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
